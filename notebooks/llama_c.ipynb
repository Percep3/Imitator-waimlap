{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6364d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1e12b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Converting and de-quantizing GGUF tensors...: 100%|██████████| 255/255 [01:21<00:00,  3.13it/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"unsloth/Llama-3.2-3B-Instruct-GGUF\"\n",
    "filename = \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa31e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pygguf' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/giorgio6846/Code/Sign-AI/Sign-chris/notebooks/pygguf\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages (from gguf==0.1.0) (2.3.1)\n",
      "Building wheels for collected packages: gguf\n",
      "  Building editable for gguf (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gguf: filename=gguf-0.1.0-0.editable-py3-none-any.whl size=5166 sha256=0fdbbaa9a1f09b48ff7860a3e2290e4dbbdecf85a89535ffc47b3426ee9d5c92\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0uztnxn8/wheels/b0/eb/bd/1cba46bf6571db3045945cb9fdd7ab7c0f21669dd9033615d2\n",
      "Successfully built gguf\n",
      "Installing collected packages: gguf\n",
      "  Attempting uninstall: gguf\n",
      "    Found existing installation: gguf 0.17.1\n",
      "    Uninstalling gguf-0.17.1:\n",
      "      Successfully uninstalled gguf-0.17.1\n",
      "Successfully installed gguf-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/99991/pygguf.git\n",
    "!cd pygguf && pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4a0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_cpp import Llama\n",
    "import gguf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7640baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_weights_from_gguf(model_path):\n",
    "    \"\"\"\n",
    "    Carga los pesos del embedding directamente desde un archivo GGUF.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Ruta al archivo .gguf del modelo.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Un array 2D con los pesos del embedding (vocab_size, embedding_dim).\n",
    "                       Retorna None si no se encuentra el tensor.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Cargar el archivo GGUF\n",
    "        reader = gguf.GGUFReader(model_path, 'r')\n",
    "\n",
    "        # 2. Iterar por los tensores para encontrar el de embedding\n",
    "        # El nombre puede variar, pero suele ser 'token_embd.weight'\n",
    "        embedding_tensor_name = 'token_embd.weight' # Nombre común\n",
    "        found_tensor = None\n",
    "\n",
    "        for tensor in reader.tensors:\n",
    "            if tensor.name == embedding_tensor_name:\n",
    "                print(f\"Tensor encontrado: {tensor.name}, Shape: {tensor.shape}, Tipo: {tensor.tensor_type}\")\n",
    "                found_tensor = tensor\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"No se encontró el tensor de embedding con el nombre '{embedding_tensor_name}' en {model_path}\")\n",
    "\n",
    "        # 3. Extraer los datos\n",
    "        # gguf.Reader nos da los datos ya deserializados y descuantizados (si es necesario)\n",
    "        # como un numpy array. El acceso es mediante found_tensor.data\n",
    "        print(found_tensor)\n",
    "        embedding_weights = found_tensor.data\n",
    "        print(embedding_weights[0])\n",
    "\n",
    "        # Asegurarse de que es un array 2D\n",
    "        # La forma típica es (n_vocab, n_embd) en el archivo,\n",
    "        # aunque internamente ggml/gguf puede almacenarlo transpuesto.\n",
    "        # gguf.Reader debería manejar esto y devolver la forma correcta.\n",
    "        # Verifica la forma:\n",
    "        print(f\"Pesos del embedding cargados. Shape: {embedding_weights.shape}, Dtype: {embedding_weights.dtype}\")\n",
    "\n",
    "        return embedding_weights\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: No se pudo encontrar el archivo {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo GGUF: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3882063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_from_gguf(model_path: str) -> np.ndarray:\n",
    "    # 1) Abrir el archivo en modo binario\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        info, tensorinfo = gguf.load_gguf(f)  # metadata + descriptores :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "        # 2) Cargar el tensor 'token_embd.weight' directamente\n",
    "        try:\n",
    "            weights = gguf.load_gguf_tensor(f, tensorinfo, \"token_embd.weight\")\n",
    "        except KeyError:\n",
    "            raise ValueError(\"No se encontró 'token_embd.weight' en tensorinfo\")\n",
    "\n",
    "    # 3) Verificar dimensiones\n",
    "    #    weights.shape debería ser (3072, vocab_size) o (vocab_size, 3072)\n",
    "    if weights.shape[0] == info.get(\"llama.n_embd\", 3072):\n",
    "        # Si viene como (3072, V), lo transponemos\n",
    "        weights = weights.T\n",
    "\n",
    "    print(f\"Embedding final: shape={weights.shape}, dtype={weights.dtype}\")\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5c2491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding final: shape=(128256, 3072), dtype=float32\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/giorgio6846/Code/Sign-AI/local_models/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
    "W = load_embedding_from_gguf(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e29a6011",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GGUFReader' object has no attribute 'tensor_by_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Ejemplo de uso ---\u001b[39;00m\n\u001b[32m      2\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33m/home/giorgio6846/Code/Sign-AI/local_models/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q4_K_M.gguf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m embedding_weights = \u001b[43mload_embedding_weights_from_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedding_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mload_embedding_weights_from_gguf\u001b[39m\u001b[34m(model_path)\u001b[39m\n\u001b[32m      2\u001b[39m reader = gguf.GGUFReader(model_path, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Obtén directamente el tensor usando su nombre:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tensor = \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor_by_name\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mtoken_embd.weight\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo se encontró token_embd.weight\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'GGUFReader' object has no attribute 'tensor_by_name'"
     ]
    }
   ],
   "source": [
    "# --- Ejemplo de uso ---\n",
    "model_path = \"/home/giorgio6846/Code/Sign-AI/local_models/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
    "embedding_weights = load_embedding_weights_from_gguf(model_path)\n",
    "#\n",
    "if embedding_weights is not None:\n",
    "    print(f\"Shape final: {embedding_weights.shape}\")\n",
    "else:\n",
    "    print(\"No se pudieron cargar los pesos del embedding.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845e1d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (1024) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    }
   ],
   "source": [
    "path_model = \"/home/giorgio6846/Code/Sign-AI/local_models/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
    "model_cpp = Llama(model_path=path_model, n_ctx=1024, embedding=True, logits_all=True, verbose=False,) #n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d6e88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3072])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = \"un texto normal como cualquiera\"\n",
    "embeddings_cpp = torch.tensor(model_cpp.embed(target))\n",
    "embeddings_cpp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c35c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(target, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cpu\")\n",
    "embeddings = model.get_input_embeddings()(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1c0af2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_cpp == embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ace3036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3eb8362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_to_text_gpu(embeddings: torch.Tensor, model, tokenizer) -> str:\n",
    "    device = \"cpu\" # torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # model.eval()\n",
    "\n",
    "    embeddings = embeddings.to(device)\n",
    "\n",
    "    # embedding_layer = model.get_input_embeddings()\n",
    "    embedding_matrix = torch.tensor(W, dtype=torch.bfloat16).to(device) # embedding_layer.weight.to(device)  # [vocab_size, hidden_dim]\n",
    "\n",
    "    embedding_matrix_norm = F.normalize(embedding_matrix, p=2, dim=1)  # [V, D]\n",
    "    print(embedding_matrix_norm.shape)\n",
    "\n",
    "    embeddings_norm = F.normalize(embeddings, p=2, dim=1)  # [T, D]\n",
    "\n",
    "    similarities = torch.matmul(embeddings_norm, embedding_matrix_norm.T)  # [T, V]\n",
    "\n",
    "    token_ids = torch.argmax(similarities, dim=1).tolist()\n",
    "    print(token_ids)\n",
    "\n",
    "    return tokenizer.decode(token_ids,) #skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ea69057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(W).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29ae6e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theited de\n",
      " elquiera de\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def embeddings_to_text(embeddings: torch.Tensor, W: np.ndarray, tokenizer) -> str:\n",
    "    # 1) fuerza float32\n",
    "    E = embeddings.to(torch.float32)            # [T, D]\n",
    "    M = torch.from_numpy(W.astype(np.float32))  # [V, D]\n",
    "\n",
    "    # 2) compara con producto punto\n",
    "    sims = E @ M.T                               # [T, V]\n",
    "    token_ids = sims.argmax(dim=1).tolist()\n",
    "\n",
    "    return tokenizer.decode(token_ids)\n",
    "\n",
    "# uso:\n",
    "# W ya cargada con load_gguf_tensor → float32 (vocab_size, 3072)\n",
    "emb = torch.tensor(model_cpp.embed(target))      # [T, D]\n",
    "text = embeddings_to_text(emb, W, model_cpp.tokenizer_)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f8fc1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128256, 3072])\n",
      "[791, 1639, 15482, 271, 10566, 447, 26919, 11158]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Theited sobre\\n\\n estequiera más'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_to_text_gpu(embeddings_cpp.to(dtype=torch.bfloat16), model_cpp, model_cpp.tokenizer_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
