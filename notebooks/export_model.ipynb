{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7d0dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853e7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TORCH_LOGS\"] = \"+dynamic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ef26aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.7.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\n",
      "Torch version 2.7.1+cu128 has not been tested with coremltools. You may run into unexpected errors. Torch 2.5.0 is the most recent version that has been tested.\n",
      "/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
      "WARNING:coremltools:Failed to load _MLCPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
      "WARNING:coremltools:Failed to load _MLGPUComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
      "WARNING:coremltools:Failed to load _MLNeuralEngineComputeDeviceProxy: No module named 'coremltools.libcoremlpython'\n",
      "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
      "WARNING:coremltools:Failed to load _MLComputePlanProxy: No module named 'coremltools.libcoremlpython'\n",
      "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
      "WARNING:coremltools:Failed to load _MLModelAssetProxy: No module named 'coremltools.libcoremlpython'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import coremltools as ct\n",
    "\n",
    "from src.mslm.utils.setup_train import build_model\n",
    "from src.mslm.utils.config_loader import ConfigLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14238eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_size': 3072,\n",
       " 'hidden_size': 1792,\n",
       " 'nhead': 16,\n",
       " 'ff_dim': 2816,\n",
       " 'n_layers': 10,\n",
       " 'encoder_dropout': 0.45,\n",
       " 'multihead_dropout': 0.4,\n",
       " 'sequential_dropout': 0.6,\n",
       " 'pool_dim': 256,\n",
       " 'input_size': 266}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_parameters = ConfigLoader(\"../config/model/config.toml\").load_config()\n",
    "model_parameters.update({\n",
    "    \"input_size\": 133 * 2,\n",
    "    \"output_size\": 3072,\n",
    "    #\"use_checkpoint\": False\n",
    "})\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "version = 106\n",
    "checkpoint = 1\n",
    "epoch = 9\n",
    "\n",
    "model_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15df59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model = build_model(**model_parameters)    \n",
    "\n",
    "    model_location = f\"../../outputs/checkpoints/{version}/{checkpoint}/{epoch}/checkpoint.pth\" \n",
    "    if not os.path.exists(model_location):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Model not found {model_location}\")\n",
    "\n",
    "    state_dict = torch.load(model_location, weights_only=False)\n",
    "\n",
    "    model.load_state_dict(state_dict[\"model_state\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0c0c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:  {'input_size': 266, 'hidden_size': 1792, 'output_size': 3072, 'nhead': 16, 'ff_dim': 2816, 'n_layers': 10, 'max_seq_length': 301, 'pool_dim': 256, 'encoder_dropout': 0.45, 'multihead_dropout': 0.4, 'sequential_dropout': 0.6}\n",
      "MHARoPE kwargs {'device': None, 'dtype': None}\n",
      "dim: 1792 num_heads: 16 dim rope 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imitator(\n",
      "  (linear_feat): Sequential(\n",
      "    (0): Linear(in_features=266, out_features=1792, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=1792, out_features=896, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (conv1): Conv1d(896, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (act1): GELU(approximate='none')\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (act2): GELU(approximate='none')\n",
      "  (linear_hidden): Linear(in_features=256, out_features=1792, bias=True)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x TransformerEncoderLayerRoPE(\n",
      "        (self_attn): MultiheadAttentionRoPE(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
      "          (rotary_pos_emb): RotaryPositionalEmbeddings()\n",
      "        )\n",
      "        (linear1): Linear(in_features=1792, out_features=2816, bias=True)\n",
      "        (dropout): Dropout(p=0.45, inplace=False)\n",
      "        (linear2): Linear(in_features=2816, out_features=1792, bias=True)\n",
      "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proj): Linear(in_features=1792, out_features=3072, bias=True)\n",
      "  (cross_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
      "  )\n",
      "  (norm_attn): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "  (proj_final): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=6144, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.6, inplace=False)\n",
      "    (3): Linear(in_features=6144, out_features=3072, bias=True)\n",
      "  )\n",
      ")\n",
      "289.53 M parameters\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'OptimizedModule' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_test = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m model_test.to(device).eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m      7\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel not found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m state_dict = torch.load(model_location, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model.load_state_dict(\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_state\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[31mTypeError\u001b[39m: 'OptimizedModule' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model_test = load_model()\n",
    "model_test.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6459c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.randn(1, 179, 133, 2).to(device)\n",
    "example_input_mask = torch.zeros((1, 179), dtype=torch.bool).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc38114",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model_test(example_input, example_input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c93107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0317,  0.1019, -0.1475,  ..., -0.0188, -0.1541,  0.0189],\n",
       "         [-0.2179, -0.2696,  2.0419,  ..., -0.8005,  0.0937,  0.9494],\n",
       "         [-0.2375, -0.5725,  1.0683,  ..., -0.0072, -1.1862,  0.6337],\n",
       "         ...,\n",
       "         [-0.1980,  0.4222,  0.0597,  ..., -0.8337, -0.1530, -0.3015],\n",
       "         [ 0.0068,  0.0682,  0.4659,  ..., -0.0224, -0.2580, -0.1173],\n",
       "         [ 0.2847,  0.5836,  0.6099,  ..., -0.6654, -0.8022,  1.0683]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cpu = output.cpu()\n",
    "output_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7749c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 3072])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cpu.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2852e876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.03165762,  0.10191493, -0.14751945, ..., -0.01876208,\n",
       "         -0.15405497,  0.01890181],\n",
       "        [-0.21793239, -0.26959455,  2.0419297 , ..., -0.8005126 ,\n",
       "          0.09369794,  0.9493704 ],\n",
       "        [-0.23749627, -0.5724557 ,  1.068256  , ..., -0.00720272,\n",
       "         -1.1861858 ,  0.63368315],\n",
       "        ...,\n",
       "        [-0.19800192,  0.42222905,  0.05972207, ..., -0.83373713,\n",
       "         -0.1530048 , -0.30145833],\n",
       "        [ 0.00675472,  0.06821718,  0.46587744, ..., -0.0224178 ,\n",
       "         -0.25804543, -0.11734705],\n",
       "        [ 0.28465277,  0.5835949 ,  0.60991955, ..., -0.66544235,\n",
       "         -0.8021887 ,  1.0683386 ]]], shape=(1, 30, 3072), dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_np = output_cpu.numpy()\n",
    "output_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9457c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([262400, 2048])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model = torch.load(\"../../local_models/gemma_W_embeds/gemma_embedding_matrix.pt\")\n",
    "gemma_model.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53e1e2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:  {'input_size': 266, 'hidden_size': 1792, 'output_size': 3072, 'nhead': 16, 'ff_dim': 2304, 'n_layers': 8, 'max_seq_length': 30, 'pool_dim': 512, 'encoder_dropout': 0.1, 'multihead_dropout': 0.35, 'sequential_dropout': 0.2}\n",
      "MHARoPE kwargs {'device': None, 'dtype': None}\n",
      "dim: 1792 num_heads: 16 dim rope 112\n",
      "Imitator(\n",
      "  (linear_feat): Sequential(\n",
      "    (0): Linear(in_features=266, out_features=1792, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=1792, out_features=896, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (conv1): Conv1d(896, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (act1): GELU(approximate='none')\n",
      "  (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "  (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (act2): GELU(approximate='none')\n",
      "  (linear_hidden): Linear(in_features=512, out_features=1792, bias=True)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerEncoderLayerRoPE(\n",
      "        (self_attn): MultiheadAttentionRoPE(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
      "          (rotary_pos_emb): RotaryPositionalEmbeddings()\n",
      "        )\n",
      "        (linear1): Linear(in_features=1792, out_features=2304, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2304, out_features=1792, bias=True)\n",
      "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proj): Linear(in_features=1792, out_features=3072, bias=True)\n",
      "  (cross_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n",
      "  )\n",
      "  (norm_attn): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "  (proj_final): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=6144, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=6144, out_features=3072, bias=True)\n",
      "  )\n",
      ")\n",
      "254.74 M parameters\n"
     ]
    }
   ],
   "source": [
    "output_location = f\"../../outputs/model_exports/{version}/{checkpoint}/{epoch}\"\n",
    "os.makedirs(output_location, exist_ok=True)\n",
    "\n",
    "model_iphone = load_model().to(\"cpu\")\n",
    "model_iphone.eval()\n",
    "\n",
    "example_input = example_input.to(\"cpu\") \n",
    "example_input_mask = example_input_mask.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c0373",
   "metadata": {},
   "source": [
    "# Convert to Executorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e7b70bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0728 18:16:50.731000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:3334] [0/0] create_env\n",
      "V0728 18:16:50.750000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6071] [0/0] _update_var_to_range s0 = VR[17, 180] (update)\n",
      "I0728 18:16:50.750000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4606] [0/0] create_symbol s0 = 179 for L['x'].size()[1] [17, 180] (_dynamo/variables/builder.py:3033 in <lambda>), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s0\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\"\n",
      "V0728 18:16:50.752000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:7018] [0/0] runtime_assert True == True [statically known]\n",
      "V0728 18:16:50.758000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:7018] [0/0] runtime_assert True == True [statically known]\n",
      "V0728 18:16:50.760000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(Ne(s0, 1)) == True [statically known]\n",
      "V0728 18:16:50.767000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(Eq(s0, 1)) == False [statically known]\n",
      "V0728 18:16:50.770000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(Ne(Mod(1, s0), 0)) == True [statically known]\n",
      "V0728 18:16:50.845000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(1 < s0) == True [statically known]\n",
      "V0728 18:16:50.853000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6071] [0/0] _update_var_to_range s1 = VR[17, 180] (update)\n",
      "I0728 18:16:50.853000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4606] [0/0] create_symbol s1 = 179 for L['frames_padding_mask'].size()[1] [17, 180] return self.transformer(x, src_key_padding_mask=frames_padding_mask)  # Code/Sign-AI/Sign-Giorgio/src/mslm/models/imitator.py:101 in transformer_checkpoint (_dynamo/variables/builder.py:3033 in <lambda>), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s1\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\"\n",
      "V0728 18:16:50.903000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(Ne(s1, 1)) == True [statically known]\n",
      "V0728 18:16:50.905000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(Eq(s1, 1)) == False [statically known]\n",
      "V0728 18:16:50.923000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(16 < s0) == True [statically known]\n",
      "V0728 18:16:50.926000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(Eq(s0, 16)) == False [statically known]\n",
      "V0728 18:16:50.927000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(Ne(Mod(16, s0), 0)) == True [statically known]\n",
      "V0728 18:16:50.928000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(112*s0 > 112) == True [statically known]\n",
      "V0728 18:16:50.937000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(s0 > 4096) == False [statically known]\n",
      "V0728 18:16:50.939000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(448*s0 > 1835008) == False [statically known]\n",
      "V0728 18:16:50.943000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(Eq(1, s0)) == False [statically known]\n",
      "V0728 18:16:50.951000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(448*s0 - 4 > 1835008) == False [statically known]\n",
      "V0728 18:16:50.954000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(s0 > 1) == True [statically known]\n",
      "I0728 18:16:50.989000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6630] [0/0] runtime_assert Eq(s1, s0) [guard added] return self.transformer(x, src_key_padding_mask=frames_padding_mask)  # Code/Sign-AI/Sign-Giorgio/src/mslm/models/imitator.py:101 in transformer_checkpoint (nn/functional.py:5989 in _check_key_padding_mask), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s1, s0)\"\n",
      "I0728 18:16:50.989000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6234] [0/0] set_replacement s1 = s0 (solve) VR[17, 180]\n",
      "I0728 18:16:50.996000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6630] [0/0] eval Ne(Mod(s0, 16), 0) [guard added] return self.transformer(x, src_key_padding_mask=frames_padding_mask)  # Code/Sign-AI/Sign-Giorgio/src/mslm/models/imitator.py:101 in transformer_checkpoint (_refs/__init__.py:3785 in _reshape_view_helper), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Ne(Mod(s0, 16), 0)\"\n",
      "V0728 18:16:50.999000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(Ne(16*s0, 16)) == True [statically known]\n",
      "V0728 18:16:51.002000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval False == True [statically known]\n",
      "V0728 18:16:51.005000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:6787] [0/0] eval size_oblivious(Eq(112*s0, 112)) == False [statically known]\n",
      "I0728 18:16:51.639000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4734] [0/0] produce_guards\n",
      "V0728 18:16:51.639000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].size()[0] 1 None\n",
      "V0728 18:16:51.640000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].size()[1] s0 StrictMinMaxConstraint(warn_only=False, vr=VR[17, 180])\n",
      "V0728 18:16:51.640000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].size()[2] 133 None\n",
      "V0728 18:16:51.640000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].size()[3] 2 None\n",
      "V0728 18:16:51.640000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].stride()[0] 266*s0 None\n",
      "V0728 18:16:51.641000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].stride()[1] 266 None\n",
      "V0728 18:16:51.641000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].stride()[2] 2 None\n",
      "V0728 18:16:51.641000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].stride()[3] 1 None\n",
      "V0728 18:16:51.641000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].storage_offset() 0 None\n",
      "V0728 18:16:51.642000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['frames_padding_mask'].size()[0] 1 None\n",
      "V0728 18:16:51.642000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['frames_padding_mask'].size()[1] s0 StrictMinMaxConstraint(warn_only=False, vr=VR[17, 180])\n",
      "V0728 18:16:51.642000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['frames_padding_mask'].stride()[0] s0 None\n",
      "V0728 18:16:51.643000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['frames_padding_mask'].stride()[1] 1 None\n",
      "V0728 18:16:51.643000 1607792 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['frames_padding_mask'].storage_offset() 0 None\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0] Error while creating guard:\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0] Name: ''\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]     Source: shape_env\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]     Create Function: SHAPE_ENV\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]     Guard Types: None\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]     Code List: None\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]     Object Weakref: None\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]     Guarded Class Weakref: None\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0] Traceback (most recent call last):\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]   File \"/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_guards.py\", line 357, in create\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]     return self.create_fn(builder, self)\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]   File \"/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/guards.py\", line 1968, in SHAPE_ENV\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]     python_code_parts, verbose_code_parts = _get_code_parts(\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]                                             ^^^^^^^^^^^^^^^^\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]   File \"/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/guards.py\", line 1951, in _get_code_parts\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]     return output_graph.shape_env.produce_guards_verbose(\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]   File \"/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5409, in produce_guards_verbose\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]     raise ConstraintViolationError(\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0] torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (seq_len)! For more information, run with TORCH_LOGS=\"+dynamic\".\n",
      "E0728 18:16:51.645000 1607792 site-packages/torch/_guards.py:359] [0/0]   - Not all values of seq_len = L['x'].size()[1] in the specified range 17 <= seq_len <= 180 satisfy the generated guard (L['x'].size()[1] % 16) != 0.\n",
      "E0728 18:16:51.656000 1607792 site-packages/torch/_guards.py:361] [0/0] Created at:\n",
      "E0728 18:16:51.656000 1607792 site-packages/torch/_guards.py:361] [0/0]   File \"/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 694, in transform\n",
      "E0728 18:16:51.656000 1607792 site-packages/torch/_guards.py:361] [0/0]     tracer = InstructionTranslator(\n",
      "E0728 18:16:51.656000 1607792 site-packages/torch/_guards.py:361] [0/0]   File \"/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3327, in __init__\n",
      "E0728 18:16:51.656000 1607792 site-packages/torch/_guards.py:361] [0/0]     output=OutputGraph(\n",
      "E0728 18:16:51.656000 1607792 site-packages/torch/_guards.py:361] [0/0]   File \"/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 358, in __init__\n",
      "E0728 18:16:51.656000 1607792 site-packages/torch/_guards.py:361] [0/0]     self.init_ambient_guards()\n",
      "E0728 18:16:51.656000 1607792 site-packages/torch/_guards.py:361] [0/0]   File \"/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 512, in init_ambient_guards\n",
      "E0728 18:16:51.656000 1607792 site-packages/torch/_guards.py:361] [0/0]     self.guards.add(ShapeEnvSource().make_guard(GuardBuilder.SHAPE_ENV))\n"
     ]
    },
    {
     "ename": "UserError",
     "evalue": "Constraints violated (seq_len)! For more information, run with TORCH_LOGS=\"+dynamic\".\n  - Not all values of seq_len = L['x'].size()[1] in the specified range 17 <= seq_len <= 180 satisfy the generated guard (L['x'].size()[1] % 16) != 0.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConstraintViolationError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/_trace.py:740\u001b[39m, in \u001b[36m_export_to_torch_ir\u001b[39m\u001b[34m(f, args, kwargs, dynamic_shapes, preserve_module_call_signature, disable_constraint_solver, allow_complex_guards_as_runtime_asserts, restore_fqn, _log_export_usage, same_signature)\u001b[39m\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx, _ignore_backend_decomps():\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m         gm_torch_level, _ = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dynamo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    743\u001b[39m \u001b[43m            \u001b[49m\u001b[43massume_static_by_default\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtracing_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msymbolic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdisable_constraint_solver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_constraint_solver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# currently the following 2 flags are tied together for export purposes,\u001b[39;49;00m\n\u001b[32m    747\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# but untangle for sake of dynamo export api\u001b[39;49;00m\n\u001b[32m    748\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefer_deferred_runtime_asserts_over_guards\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_log_export_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_log_export_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m            \u001b[49m\u001b[43msame_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msame_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ConstraintViolationError, ValueRangeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1722\u001b[39m, in \u001b[36mexport.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m constraint_violation_error:\n\u001b[32m-> \u001b[39m\u001b[32m1722\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m constraint_violation_error\n\u001b[32m   1724\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1677\u001b[39m, in \u001b[36mexport.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1676\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1677\u001b[39m     result_traced = \u001b[43mopt_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConstraintViolationError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:655\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1432\u001b[39m, in \u001b[36mCatchErrorsWrapper.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, frame_state)\u001b[39m\n\u001b[32m   1430\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[32m   1431\u001b[39m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1434\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:598\u001b[39m, in \u001b[36mConvertFrameAssert.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1059\u001b[39m, in \u001b[36m_compile\u001b[39m\u001b[34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m     guarded_code = \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1061\u001b[39m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[32m   1062\u001b[39m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[32m   1063\u001b[39m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_utils_internal.py:97\u001b[39m, in \u001b[36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler.enabled:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler.profile_compile_time(\n\u001b[32m    100\u001b[39m     function, phase_name, *args, **kwargs\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:761\u001b[39m, in \u001b[36m_compile.<locals>.compile_inner\u001b[39m\u001b[34m(code, one_graph, hooks, transform)\u001b[39m\n\u001b[32m    760\u001b[39m     stack.enter_context(CompileTimeInstructionCounter.record())\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    764\u001b[39m     ConvertFrameReturn()\n\u001b[32m    765\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:906\u001b[39m, in \u001b[36m_compile.<locals>._compile_inner\u001b[39m\u001b[34m(code, one_graph, hooks, transform)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mnonlocal\u001b[39;00m cache_entry\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m check_fn = \u001b[43mCheckFunctionManager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mguard_fail_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    913\u001b[39m compile_id_str = \u001b[38;5;28mstr\u001b[39m(compile_id) \u001b[38;5;28;01mif\u001b[39;00m compile_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUnknown\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/guards.py:2490\u001b[39m, in \u001b[36mCheckFunctionManager.__init__\u001b[39m\u001b[34m(self, f_code, output_graph, cache_entry, guard_fail_fn)\u001b[39m\n\u001b[32m   2488\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2490\u001b[39m     \u001b[43mguard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2492\u001b[39m \u001b[38;5;28mself\u001b[39m.compile_check_fn(builder, guards, guard_fail_fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_guards.py:357\u001b[39m, in \u001b[36mGuard.create\u001b[39m\u001b[34m(self, builder)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/guards.py:1968\u001b[39m, in \u001b[36mGuardBuilder.SHAPE_ENV\u001b[39m\u001b[34m(self, guard)\u001b[39m\n\u001b[32m   1967\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m     python_code_parts, verbose_code_parts = \u001b[43m_get_code_parts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbose_python\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[38;5;66;03m# When exporting, we may work with the shape constraints some more in\u001b[39;00m\n\u001b[32m   1973\u001b[39m \u001b[38;5;66;03m# postprocessing, so don't freeze yet\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/_dynamo/guards.py:1951\u001b[39m, in \u001b[36mGuardBuilder.SHAPE_ENV.<locals>._get_code_parts\u001b[39m\u001b[34m(langs)\u001b[39m\n\u001b[32m   1950\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_code_parts\u001b[39m(langs):\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moutput_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproduce_guards_verbose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfake\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_contexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_contexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mequalities_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequalities_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1956\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1957\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Export keeps static.\u001b[39;49;00m\n\u001b[32m   1958\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_static\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_fn_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1959\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlangs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlangs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py:5409\u001b[39m, in \u001b[36mShapeEnv.produce_guards_verbose\u001b[39m\u001b[34m(self, placeholders, sources, source_ref, guards, input_contexts, equalities_inputs, _simplified, ignore_static, langs)\u001b[39m\n\u001b[32m   5408\u001b[39m     err = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m-> \u001b[39m\u001b[32m5409\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ConstraintViolationError(\n\u001b[32m   5410\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConstraints violated (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdebug_names_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)! \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5411\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mFor more information, run with TORCH_LOGS=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m+dynamic\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   5412\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   5413\u001b[39m     )\n\u001b[32m   5414\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(warn_msgs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[31mConstraintViolationError\u001b[39m: Constraints violated (seq_len)! For more information, run with TORCH_LOGS=\"+dynamic\".\n  - Not all values of seq_len = L['x'].size()[1] in the specified range 17 <= seq_len <= 180 satisfy the generated guard (L['x'].size()[1] % 16) != 0.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUserError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export, Dim\n\u001b[32m      3\u001b[39m dynamic_shapes = {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m      5\u001b[39m         \u001b[32m1\u001b[39m: Dim(\u001b[33m\"\u001b[39m\u001b[33mseq_len\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mmin\u001b[39m=\u001b[32m17\u001b[39m, \u001b[38;5;28mmax\u001b[39m=\u001b[32m180\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     }\n\u001b[32m     10\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m exported_program = \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_iphone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_input_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/__init__.py:360\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch.jit.ScriptModule):\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    356\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExporting a ScriptModule is not supported. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    357\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMaybe try converting your ScriptModule to an ExportedProgram \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    358\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33musing `TS2EPConverter(mod, args, kwargs).convert()` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    359\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/_trace.py:1100\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1094\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mpartial_fx_graph\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1095\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m   1096\u001b[39m             e.partial_fx_graph,\n\u001b[32m   1097\u001b[39m             file=sys.stderr,\n\u001b[32m   1098\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     _EXPORT_FLAGS = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/_trace.py:1066\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     start = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m     ep = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m     end = time.time()\n\u001b[32m   1068\u001b[39m     log_export_usage(\n\u001b[32m   1069\u001b[39m         event=\u001b[33m\"\u001b[39m\u001b[33mexport.time\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1070\u001b[39m         metrics=end - start,\n\u001b[32m   1071\u001b[39m         flags=_EXPORT_FLAGS,\n\u001b[32m   1072\u001b[39m         **get_ep_stats(ep),\n\u001b[32m   1073\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/exported_program.py:121\u001b[39m, in \u001b[36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/_trace.py:2120\u001b[39m, in \u001b[36m_export\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, pre_dispatch, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace)\u001b[39m\n\u001b[32m   2112\u001b[39m \u001b[38;5;66;03m# NOTE Export training IR rollout\u001b[39;00m\n\u001b[32m   2113\u001b[39m \u001b[38;5;66;03m# Old export calls export._trace(pre_dispatch=True)\u001b[39;00m\n\u001b[32m   2114\u001b[39m \u001b[38;5;66;03m# and there are still lot of internal/OSS callsites that\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2117\u001b[39m \u001b[38;5;66;03m# export_training_ir_rollout_check returns True in OSS\u001b[39;00m\n\u001b[32m   2118\u001b[39m \u001b[38;5;66;03m# while internally it returns False UNLESS otherwise specified.\u001b[39;00m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;129;01mand\u001b[39;00m export_training_ir_rollout_check():\n\u001b[32m-> \u001b[39m\u001b[32m2120\u001b[39m     ep = \u001b[43m_export_for_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2127\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2128\u001b[39m     dtrace_structured(\u001b[33m\"\u001b[39m\u001b[33mexported_program\u001b[39m\u001b[33m\"\u001b[39m, payload_fn=\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mstr\u001b[39m(ep))\n\u001b[32m   2129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ep\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/_trace.py:1100\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1094\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mpartial_fx_graph\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1095\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m   1096\u001b[39m             e.partial_fx_graph,\n\u001b[32m   1097\u001b[39m             file=sys.stderr,\n\u001b[32m   1098\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     _EXPORT_FLAGS = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/_trace.py:1066\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     start = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m     ep = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m     end = time.time()\n\u001b[32m   1068\u001b[39m     log_export_usage(\n\u001b[32m   1069\u001b[39m         event=\u001b[33m\"\u001b[39m\u001b[33mexport.time\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1070\u001b[39m         metrics=end - start,\n\u001b[32m   1071\u001b[39m         flags=_EXPORT_FLAGS,\n\u001b[32m   1072\u001b[39m         **get_ep_stats(ep),\n\u001b[32m   1073\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/exported_program.py:121\u001b[39m, in \u001b[36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/_trace.py:1983\u001b[39m, in \u001b[36m_export_for_training\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[39m\n\u001b[32m   1970\u001b[39m original_state_dict = _get_original_state_dict(mod)\n\u001b[32m   1972\u001b[39m export_func = (\n\u001b[32m   1973\u001b[39m     functools.partial(\n\u001b[32m   1974\u001b[39m         _strict_export_lower_to_aten_ir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1981\u001b[39m     )\n\u001b[32m   1982\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1983\u001b[39m export_artifact = \u001b[43mexport_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[operator]\u001b[39;49;00m\n\u001b[32m   1984\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1985\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1989\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moriginal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1991\u001b[39m \u001b[43m    \u001b[49m\u001b[43morig_in_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43morig_in_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1992\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_torch_jit_trace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1996\u001b[39m export_graph_signature = export_artifact.aten.sig\n\u001b[32m   1998\u001b[39m forward_arg_names = _get_forward_arg_names(mod, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/_trace.py:1352\u001b[39m, in \u001b[36m_strict_export_lower_to_aten_ir\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, preserve_module_call_signature, pre_dispatch, original_state_dict, orig_in_spec, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace, lower_to_aten_callback)\u001b[39m\n\u001b[32m   1339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_strict_export_lower_to_aten_ir\u001b[39m(\n\u001b[32m   1340\u001b[39m     mod: torch.nn.Module,\n\u001b[32m   1341\u001b[39m     args: \u001b[38;5;28mtuple\u001b[39m[Any, ...],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1350\u001b[39m     lower_to_aten_callback: Callable,\n\u001b[32m   1351\u001b[39m ) -> ExportArtifact:\n\u001b[32m-> \u001b[39m\u001b[32m1352\u001b[39m     gm_torch_level = \u001b[43m_export_to_torch_ir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrestore_fqn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't need to restore because we will do it later\u001b[39;49;00m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_log_export_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1363\u001b[39m     \u001b[38;5;66;03m# We detect the fake_mode by looking at gm_torch_level's placeholders, this is the fake_mode created in dynamo.\u001b[39;00m\n\u001b[32m   1364\u001b[39m     (\n\u001b[32m   1365\u001b[39m         fake_args,\n\u001b[32m   1366\u001b[39m         fake_kwargs,\n\u001b[32m   1367\u001b[39m         dynamo_fake_mode,\n\u001b[32m   1368\u001b[39m     ) = _extract_fake_inputs(gm_torch_level, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/export/_trace.py:757\u001b[39m, in \u001b[36m_export_to_torch_ir\u001b[39m\u001b[34m(f, args, kwargs, dynamic_shapes, preserve_module_call_signature, disable_constraint_solver, allow_complex_guards_as_runtime_asserts, restore_fqn, _log_export_usage, same_signature)\u001b[39m\n\u001b[32m    740\u001b[39m         gm_torch_level, _ = torch._dynamo.export(\n\u001b[32m    741\u001b[39m             f,\n\u001b[32m    742\u001b[39m             dynamic_shapes=dynamic_shapes,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    754\u001b[39m             **kwargs,\n\u001b[32m    755\u001b[39m         )\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ConstraintViolationError, ValueRangeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(UserErrorType.CONSTRAINT_VIOLATION, \u001b[38;5;28mstr\u001b[39m(e))  \u001b[38;5;66;03m# noqa: B904\u001b[39;00m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GuardOnDataDependentSymNode \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    759\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(  \u001b[38;5;66;03m# noqa: B904\u001b[39;00m\n\u001b[32m    760\u001b[39m         UserErrorType.ANTI_PATTERN,\n\u001b[32m    761\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConsider annotating your code using torch._check*(). \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    762\u001b[39m         case_name=\u001b[33m\"\u001b[39m\u001b[33mconstrain_as_size_example\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    763\u001b[39m     )\n",
      "\u001b[31mUserError\u001b[39m: Constraints violated (seq_len)! For more information, run with TORCH_LOGS=\"+dynamic\".\n  - Not all values of seq_len = L['x'].size()[1] in the specified range 17 <= seq_len <= 180 satisfy the generated guard (L['x'].size()[1] % 16) != 0.\n"
     ]
    }
   ],
   "source": [
    "from torch.export import export, Dim\n",
    "\n",
    "dynamic_shapes = {\n",
    "    \"x\": {\n",
    "        1: Dim(\"seq_len\", min=17, max=180),\n",
    "    },\n",
    "    \"frames_padding_mask\": {\n",
    "        1: Dim(\"seq_len\", min=17, max=180),\n",
    "    }\n",
    "}\n",
    "\n",
    "exported_program = export(model_iphone, (example_input, example_input_mask), dynamic_shapes=dynamic_shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e02a6",
   "metadata": {},
   "source": [
    "# Convert to CoreMLTools (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scripted_model = torch.jit.script(model_iphone, (example_input.to('cpu'), example_input_mask.to('cpu')))\n",
    "traced_model = torch.jit.trace(model_iphone, (example_input, example_input_mask))\n",
    "\n",
    "inputKeypoints_shape = ct.Shape(shape=(1, ct.RangeDim(lower_bound=1, upper_bound=180, default=90), 133,2))\n",
    "inputMask_shape = ct.Shape(shape=(1, ct.RangeDim(lower_bound=1, upper_bound=180, default=90)))\n",
    "\n",
    "coreml_model = ct.convert(traced_model, \n",
    "                          inputs = [\n",
    "    ct.TensorType(shape=inputKeypoints_shape, name=\"keypoints\"),\n",
    "    ct.TensorType(shape=inputMask_shape, name=\"mask\")],\n",
    "    outputs=[\n",
    "    ct.TensorType(name=\"embeddings\")],\n",
    "    convert_to=\"mlprogram\")\n",
    "\n",
    "coreml_model.save(f\"{output_location}/model.mlpackage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
