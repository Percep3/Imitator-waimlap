{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56291f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772efec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(os.path.abspath(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37dea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from src.mslm.utils.setup_train import build_model\n",
    "from src.mslm.utils.config_loader import cfg\n",
    "\n",
    "from src.mslm.inference import MultimodalSignLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cc476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 266,\n",
       " 'output_size': 3072,\n",
       " 'hidden_size': 1792,\n",
       " 'nhead': 16,\n",
       " 'ff_dim': 2816,\n",
       " 'n_layers': 10,\n",
       " 'encoder_dropout': 0.45,\n",
       " 'multihead_dropout': 0.4,\n",
       " 'sequential_dropout': 0.6,\n",
       " 'pool_dim': 256}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_parameters = cfg.model\n",
    "model_parameters.update({\n",
    "    \"input_size\": 133 * 2,\n",
    "    \"output_size\": 3072,\n",
    "    #\"use_checkpoint\": False\n",
    "})\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "version = 107\n",
    "checkpoint = 1\n",
    "epoch = 9\n",
    "\n",
    "model_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d59d2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model = build_model(**model_parameters)    \n",
    "\n",
    "    model_location = f\"../outputs/checkpoints/{version}/{checkpoint}/{epoch}/checkpoint.pth\" \n",
    "    if not os.path.exists(model_location):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Model not found {model_location}\")\n",
    "\n",
    "    state_dict = torch.load(model_location)\n",
    "\n",
    "    model.load_state_dict(state_dict[\"model_state\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a90c520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:  {'input_size': 266, 'hidden_size': 1792, 'output_size': 3072, 'nhead': 16, 'ff_dim': 2816, 'n_layers': 10, 'max_seq_length': 301, 'pool_dim': 256, 'encoder_dropout': 0.45, 'multihead_dropout': 0.4, 'sequential_dropout': 0.6}\n",
      "MHARoPE kwargs {'device': None, 'dtype': None}\n",
      "dim: 1792 num_heads: 16 dim rope 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/Sign/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imitator(\n",
      "  (linear_feat): Sequential(\n",
      "    (0): Linear(in_features=266, out_features=1792, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=1792, out_features=896, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (conv1): Conv1d(896, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (act1): GELU(approximate='none')\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (act2): GELU(approximate='none')\n",
      "  (linear_hidden): Linear(in_features=256, out_features=1792, bias=True)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x TransformerEncoderLayerRoPE(\n",
      "        (self_attn): MultiheadAttentionRoPE(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
      "          (rotary_pos_emb): RotaryPositionalEmbeddings()\n",
      "        )\n",
      "        (linear1): Linear(in_features=1792, out_features=2816, bias=True)\n",
      "        (dropout): Dropout(p=0.45, inplace=False)\n",
      "        (linear2): Linear(in_features=2816, out_features=1792, bias=True)\n",
      "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.45, inplace=False)\n",
      "        (dropout2): Dropout(p=0.45, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proj): Linear(in_features=1792, out_features=3072, bias=True)\n",
      "  (cross_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
      "  )\n",
      "  (norm_attn): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
      "  (proj_final): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=6144, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.6, inplace=False)\n",
      "    (3): Linear(in_features=6144, out_features=3072, bias=True)\n",
      "  )\n",
      ")\n",
      "289.53 M parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Imitator(\n",
       "  (linear_feat): Sequential(\n",
       "    (0): Linear(in_features=266, out_features=1792, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=1792, out_features=896, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (conv1): Conv1d(896, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (act1): GELU(approximate='none')\n",
       "  (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "  (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (act2): GELU(approximate='none')\n",
       "  (linear_hidden): Linear(in_features=256, out_features=1792, bias=True)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-9): 10 x TransformerEncoderLayerRoPE(\n",
       "        (self_attn): MultiheadAttentionRoPE(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "          (rotary_pos_emb): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (linear1): Linear(in_features=1792, out_features=2816, bias=True)\n",
       "        (dropout): Dropout(p=0.45, inplace=False)\n",
       "        (linear2): Linear(in_features=2816, out_features=1792, bias=True)\n",
       "        (norm1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.45, inplace=False)\n",
       "        (dropout2): Dropout(p=0.45, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj): Linear(in_features=1792, out_features=3072, bias=True)\n",
       "  (cross_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1792, out_features=1792, bias=True)\n",
       "  )\n",
       "  (norm_attn): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)\n",
       "  (proj_final): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=6144, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.6, inplace=False)\n",
       "    (3): Linear(in_features=6144, out_features=3072, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8920b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id=\"unsloth/Llama-3.2-3B-Instruct\"\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "embeddings = llama_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3716ec9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d8946bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mslm.utils import create_dataloaders, prepare_datasets\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "506bdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file = \"../data/processed/dataset_v4_unsloth.hdf5\"\n",
    "train_ratio = 0.8\n",
    "key_points = 133\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc57c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos:  12577\n",
      "Train size:\t10062\n",
      "Validation size:\t2515\n"
     ]
    }
   ],
   "source": [
    "tr_ds, val_ds, tr_len, val_len = prepare_datasets(h5_file, train_ratio, key_points)\n",
    "#get sample dataset\n",
    "sample_tr = Subset(tr_ds, range(0, 1))\n",
    "sample_val = Subset(val_ds, range(0, 1))\n",
    "\n",
    "sample_tr_len = len(sample_tr)\n",
    "sample_val_len = len(sample_val)\n",
    "\n",
    "_, test_dataloader = create_dataloaders(\n",
    "    sample_tr,\n",
    "    sample_val,\n",
    "    batch_size=2,  # asegúrate que ≤ len(sample)\n",
    "    num_workers=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c34b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1932, 0.1563],\n",
      "          [0.0630, 0.1526],\n",
      "          [0.0192, 0.3261],\n",
      "          ...,\n",
      "          [0.2650, 0.2704],\n",
      "          [0.2549, 0.2659],\n",
      "          [0.2448, 0.2659]],\n",
      "\n",
      "         [[0.1933, 0.1562],\n",
      "          [0.0633, 0.1526],\n",
      "          [0.0205, 0.3266],\n",
      "          ...,\n",
      "          [0.2614, 0.2670],\n",
      "          [0.2524, 0.2634],\n",
      "          [0.2434, 0.2643]],\n",
      "\n",
      "         [[0.1931, 0.1564],\n",
      "          [0.0624, 0.1528],\n",
      "          [0.0212, 0.3265],\n",
      "          ...,\n",
      "          [0.2626, 0.2668],\n",
      "          [0.2537, 0.2631],\n",
      "          [0.2448, 0.2631]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1803, 0.1527],\n",
      "          [0.0645, 0.1471],\n",
      "          [0.0338, 0.3183],\n",
      "          ...,\n",
      "          [0.3447, 0.3295],\n",
      "          [0.3595, 0.3312],\n",
      "          [0.3722, 0.3321]],\n",
      "\n",
      "         [[0.1805, 0.1524],\n",
      "          [0.0634, 0.1470],\n",
      "          [0.0261, 0.3178],\n",
      "          ...,\n",
      "          [0.3327, 0.3419],\n",
      "          [0.3459, 0.3392],\n",
      "          [0.3580, 0.3383]],\n",
      "\n",
      "         [[0.1815, 0.1539],\n",
      "          [0.0621, 0.1486],\n",
      "          [0.0167, 0.3209],\n",
      "          ...,\n",
      "          [0.3225, 0.3552],\n",
      "          [0.3355, 0.3517],\n",
      "          [0.3463, 0.3490]]]])\n"
     ]
    }
   ],
   "source": [
    "for a in test_dataloader:\n",
    "    print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db0a6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf6ea038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_model.eval()\n",
    "# mslm = MultimodalSignLM(llama_model, tokenizer, \"cuda\")\n",
    "all_embeddings = llama_model.get_input_embeddings().weight.data\n",
    "del llama_model\n",
    "# text = \"Enumera los pasos descritos:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ad243cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_to_text(embeddings: torch.Tensor, all_embeddings: torch.Tensor, tokenizer) -> str:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    embeddings = embeddings.to(device)\n",
    "\n",
    "    embedding_matrix = all_embeddings.to(device)  # [V, D]\n",
    "\n",
    "    embedding_matrix_norm = F.normalize(embedding_matrix, p=2, dim=1)  # [V, D]\n",
    "    print(embedding_matrix_norm.shape)\n",
    "\n",
    "    embeddings_norm = F.normalize(embeddings, p=2, dim=1)  # [T, D]\n",
    "\n",
    "    similarities = torch.matmul(embeddings_norm, embedding_matrix_norm.T)  # [T, V]\n",
    "\n",
    "    token_ids = torch.argmax(similarities, dim=1).tolist()\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    return tokenizer.decode(token_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b4edee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc6d20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mask_data, data, embeds, sign_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af762e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_embeds = None\n",
    "pred_embeds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12504de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([1, 100, 133, 2]), Mask shape: torch.Size([1, 100])\n",
      "Sign embed shape: torch.Size([1, 301, 3072]), Embeds shape: torch.Size([1, 23, 3072])\n",
      "torch.Size([128256, 3072])\n",
      "Token IDs: [128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000]\n",
      "\n",
      "torch.Size([128256, 3072])\n",
      "Token IDs: [128000, 3462, 346, 1744, 29571, 4247, 1645, 1995, 323, 12328, 40608, 281, 521, 4988, 13, 29386, 64591, 8374, 384, 1007, 84, 10610, 30]\n",
      "parece que también por acá anduvieron paseando. ¿qué tal epecuén?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for data, mask_data, embeds, mask_embds in test_dataloader:\n",
    "        data = data.to(device).to(torch.float32)\n",
    "        mask_data = mask_data.to(device)\n",
    "\n",
    "        print(f\"Data shape: {data.shape}, Mask shape: {mask_data.shape}\")\n",
    "        sign_embed = model(data, mask_data).to(\"cuda\")\n",
    "        sign_embed = sign_embed.to(dtype=torch.bfloat16)\n",
    "\n",
    "        sign_embed = sign_embed.to(\"cuda\").to(dtype=torch.float32)\n",
    "        embeds = embeds.to(\"cuda\").to(dtype=torch.float32)\n",
    "        print(f\"Sign embed shape: {sign_embed.shape}, Embeds shape: {embeds.shape}\")\n",
    "        \n",
    "        true_embeds = embeds\n",
    "        pred_embeds = sign_embed\n",
    "        pred_embeds = pred_embeds[:, :23, :][0]\n",
    "        print(embeddings_to_text(pred_embeds, all_embeddings, tokenizer))\n",
    "        print(embeddings_to_text(embeds[0], all_embeddings, tokenizer))\n",
    "        \n",
    "        del mask_data, data, embeds, sign_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc1b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 3072])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_embeds[:, :23, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc473113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3072])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_embeds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
