{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37dea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from Classes.train.Imitator import Imitator\n",
    "from Classes.train.PositionalEncoding import PositionalEncoding\n",
    "from Classes.dataloader import KeypointDataset, SignDataLoader, collate_fn\n",
    "from Classes.utils.llm_tools import Tools\n",
    "\n",
    "from Classes.inference import MultimodalSignLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59d2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParameters = {\n",
    "    \"input_size\": 543*2,\n",
    "    \"output_size\": 3072,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"epochs\": 1000,\n",
    "    \"logIntervals\": 20,\n",
    "    \"checkpointIntervals\": 40,\n",
    "    \"batchSize\": 32,\n",
    "    \"frameClips\": 15 * 35,\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"validation_ratio\": 0.2\n",
    "}\n",
    "# model = Imitator(input_size=modelParameters[\"input_size\"], T_size=modelParameters[\"frameClips\"], output_size=modelParameters[\"output_size\"]).to(modelParameters[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be5450a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.serialization.add_safe_globals([Imitator, PositionalEncoding])\n",
    "state_dict = torch.load(\"./model/checkpoints/33/1/15/model.pt\", weights_only=False)\n",
    "\n",
    "model_parameters = {\n",
    "    \"input_size\": 543*2,\n",
    "    \"output_size\": 3072,\n",
    "    \"ff_dim\": 1792,\n",
    "    \"n_layers\": 12,\n",
    "    \"T_size\": 15 * 35,\n",
    "}\n",
    "model = Imitator(**model_parameters)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "973395ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPath = os.path.join(os.getcwd(), os.pardir, \"data\", \"dataset2\")\n",
    "h5File = os.path.join(DataPath, \"keypoints.h5\")\n",
    "csvFile = os.path.join(DataPath, \"meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798b57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 * 2\n",
    "load_in_4bit = True\n",
    "dtype=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "493a9c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 15.576 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "llama_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e445d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = llama_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf14b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 15.576 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "LOG = False\n",
    "tools = Tools()\n",
    "keypointReader = KeypointDataset(h5Path=h5File, labelsCSV=csvFile, max_seq_len=modelParameters[\"frameClips\"])[0]\n",
    "dataset = SignDataLoader(tokenizer, [keypointReader], modelParameters[\"device\"])\n",
    "test_dataloader = DataLoader(dataset, batch_size=modelParameters[\"batchSize\"], shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99f9aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with state dict\n",
    "# model = Imitator(input_size=modelParameters[\"input_size\"], T_size=modelParameters[\"frameClips\"], output_size=modelParameters[\"output_size\"]).to(modelParameters[\"device\"])\n",
    "# model.load_state_dict(torch.load(\"./model/checkpoints/2/1/80\")[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f62c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cerrar las canillas durante el cepillado de dientes, de lavarse las manos, de la cara, de afeitarse, de lavar los platos, pelar papas, en lugar de dejar correr el agua.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypointReader[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c80811e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,  24913,    277,   5252,    649,  34344,  30331,    658,  63190,\n",
       "           484,   2172,    409,    294,  27335,     11,    409,  30583,   2648,\n",
       "          5252,  97349,     11,    409,   1208,  48034,     11,    409,    264,\n",
       "         62221,   2648,     11,    409,  30583,    277,   2537,    628,  14357,\n",
       "            11,  12077,    277,  26365,    300,     11,    665,  35000,    409,\n",
       "         81499,   1867,  38149,    658,  56562,     13, 128004, 128004, 128004,\n",
       "        128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004,\n",
       "        128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004,\n",
       "        128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004,\n",
       "        128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004,\n",
       "        128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004,\n",
       "        128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004,\n",
       "        128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004,\n",
       "        128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004,\n",
       "        128004, 128004])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd02f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90e46d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imitator(\n",
       "  (linear): Linear(in_features=1086, out_features=1024, bias=True)\n",
       "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (pe): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=1792, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1792, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (temporal_adjuster): Sequential(\n",
       "    (0): Linear(in_features=525, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (linear_out): Linear(in_features=1024, out_features=3072, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf6ea038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Enumera los pasos descritos:<|begin_of_text|>enkrvldkf la que que que la la que la la la que de de de la que la que que que que que quekrvldkf quekrvldkf quekrvldkfkrvldkfkrvldkfkrvldkfkrvldkfkrvldkfkrvldkfkrvldkfkrvldkfkrvldkfkrvldkfkrvldkfkrvldkfkrvldkfÄ±ntÄ±Ä±ntÄ±Ä±ntÄ±krvldkfï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ä±ntÄ±ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "No se puede identificar con la informaciÃ³n proporcionada, pero puedo ayudarte con algo mÃ¡s. Â¿En quÃ© puedo ayudarte?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "llama_model.eval()\n",
    "mslm = MultimodalSignLM(llama_model, tokenizer, \"cuda\")\n",
    "\n",
    "text = \"Enumera los pasos descritos:\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, embeds in test_dataloader:\n",
    "        data = data.to(modelParameters[\"device\"])\n",
    "        sign_embed = model(data).to(\"cuda\")\n",
    "        sign_embed = sign_embed.to(dtype=torch.bfloat16)\n",
    "\n",
    "        # Normaliza ambos embeddings antes de calcular similitud\n",
    "        sign_embed = F.normalize(sign_embed, dim=-1)\n",
    "        embeds = F.normalize(embeds.to(sign_embed.dtype), dim=-1)\n",
    "        \n",
    "        # similarity = torch.mean(torch.sum(sign_embed * embeds, dim=-1))  # ya que estÃ¡n normalizados\n",
    "        # print(similarity)\n",
    "        \n",
    "        print(mslm.generate(sign_embed, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7076b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547192ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model/checkpoints/2/1/80'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m model_parameters = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minput_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m543\u001b[39m*\u001b[32m2\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3072\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mT_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m15\u001b[39m * \u001b[32m35\u001b[39m,\n\u001b[32m      7\u001b[39m }\n\u001b[32m      8\u001b[39m model = Imitator(**model_parameters)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./model/checkpoints/2/1/80\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './model/checkpoints/2/1/80'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
