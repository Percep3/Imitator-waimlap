{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/Code/Sign-AI/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "max_seq_length = 2048 * 2\n",
    "dtype = None\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener capa de embeddings\n",
    "embedding_layer = model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def dispair():\n",
    "    global model\n",
    "    del model\n",
    "    \n",
    "    while True:\n",
    "        torch.cuda.empty_cache()\n",
    "        if gc.collect() == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_embed = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = embedding_layer.weight.data.to(device_embed)  # Tensor de forma [vocab_size, d_model]\n",
    "vocab_size, d_model = all_embeddings.shape\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}, d_model: {d_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_token(embedding, all_embeddings):\n",
    "    embedding = embedding.to(device_embed)\n",
    "    if embedding.dim() > 1:\n",
    "        embedding = embedding.squeeze()\n",
    "\n",
    "    # Calcular similitud del coseno\n",
    "    similarities = F.cosine_similarity(embedding.unsqueeze(0), all_embeddings, dim=1)\n",
    "\n",
    "    # Encontrar el √≠ndice del token m√°s similar\n",
    "    closest_token_id = torch.argmax(similarities).item()\n",
    "    return closest_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Token-to-Embedding-to-Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabra = \"hola\"\n",
    "tokens = tokenizer(palabra, return_tensors=\"pt\")\n",
    "token_ids = tokens[\"input_ids\"]\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "embeddings = embedding_layer(token_ids.to(\"cuda\"))\n",
    "print(f\"Embeddings: {embeddings}\\nEmbeddings shape: {embeddings.shape}\")\n",
    "\n",
    "for emb in embeddings[0]:  # embeddings[0] porque es un batch de tama√±o 1\n",
    "    closest_token_id = find_closest_token(emb, all_embeddings)\n",
    "    closest_token = tokenizer.decode([closest_token_id])\n",
    "    print(f\"Token m√°s cercano: {closest_token} (ID: {closest_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from Classes.SignDataLoader import SignDataLoader\n",
    "from Classes.Imitator import Imitator\n",
    "from Classes.KeypointDataset import KeypointDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"<|finetune_right_pad_id|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs=100, log_interval=10, learning_rate=1e-4):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    writer = SummaryWriter(\"imitator_report\")\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"epoch\", \"loss\"])\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Entrenando\", colour=\"green\"):\n",
    "        total_loss = 0\n",
    "        for data, embeddings in train_loader:\n",
    "            data = data.to(\"cuda\")\n",
    "            embeddings = embeddings.to(\"cuda\")\n",
    "\n",
    "            output = model(data)\n",
    "            #print(output.shape)\n",
    "            loss = criterion(output, embeddings)\n",
    "            total_loss += loss\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            df.loc[len(df)] = [epoch, f\"{total_loss/len(train_loader):.4f}\"]\n",
    "            clear_output()\n",
    "            print(\"Epoch: \", epoch, \".\\t Total loss: \", total_loss/len(train_loader))\n",
    "            display(df)\n",
    "    \n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPath = os.path.join(os.getcwd(), os.pardir, \"data\", \"dataset2\")\n",
    "h5File = os.path.join(DataPath, \"keypoints.h5\")\n",
    "csvFile = os.path.join(DataPath, \"meta.csv\")\n",
    "\n",
    "LIMITS_SECONDS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "input_size = 543*2 # cantidad de puntos x 2\n",
    "output_size = 3072\n",
    "learning_rate = 2e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypointReader = KeypointDataset(h5Path=h5File, labelsCSV=csvFile, max_seq_len=LIMITS_SECONDS * 35)\n",
    "dataset = SignDataLoader(tokenizer, embedding_layer, keypointReader, device)\n",
    "dataloader = DataLoader(dataset, batch_size=12, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "dispair()\n",
    "model = Imitator(input_size=input_size, output_size=output_size, d_model=d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataloader, epochs=100, log_interval=10, learning_rate=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
