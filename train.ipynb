{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "from Classes.SignDataLoader import SignDataLoader\n",
    "from Classes.Imitator import Imitator\n",
    "from Classes.KeypointDataset import KeypointDataset\n",
    "from Classes.Tools import Tools\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = pad_sequence(item[0] for item in batch)\n",
    "    data = data.permute(1, 0, 2, 3)\n",
    "\n",
    "    embeddings = pad_sequence((item[1] for item in batch), padding_value=128004)\n",
    "    embeddings = embeddings.permute(1, 0, 2)\n",
    "\n",
    "    if LOG:\n",
    "        print(f\"Data: {data.size()}, Embeddings: {embeddings.size()}\")\n",
    "\n",
    "    return data, embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    modelVersions,\n",
    "    modelDir, \n",
    "    epochs=100,\n",
    "    log_interval=10,\n",
    "    checkpoint_interval=5,\n",
    "    learning_rate=1e-4,\n",
    "    device = \"cuda\"\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, fused=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    writer = SummaryWriter(\"imitator_report\")\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"epoch\", \"loss\"])\n",
    "    \n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Entrenando\", colour=\"green\"):\n",
    "        total_loss = 0\n",
    "        for data, embeddings in train_loader:\n",
    "            if LOG: \n",
    "                print(data.shape) #[12, 1050, 543, 2]\n",
    "            if LOG: \n",
    "                print(embeddings.shape) #[12, 128, 3072]\n",
    "\n",
    "            with torch.autograd.profiler.record_function(\"Data to CUDA\"):\n",
    "                data = data.to(device)\n",
    "                embeddings = embeddings.to(device)\n",
    "            \n",
    "            if torch.cuda.get_device_capability()[0] >= 8:\n",
    "                with autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, embeddings)\n",
    "            else:\n",
    "                with autocast(device_type=device):\n",
    "                    output = model(data)\n",
    "                    output = output.to(torch.bfloat16)\n",
    "                    loss = criterion(output, embeddings)\n",
    "\n",
    "            if LOG: \n",
    "                print(output.shape)\n",
    "                print(output[0][0])\n",
    "\n",
    "                print(\"Model Output: \", output)\n",
    "                print(\"Model Embeddings: \", embeddings)\n",
    "\n",
    "            total_loss += loss\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 1)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            df.loc[len(df)] = [epoch, f\"{total_loss/len(train_loader):.4f}\"]\n",
    "            \n",
    "        if epoch % checkpoint_interval == 0 and epoch != 0: \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': total_loss\n",
    "            }, os.path.join(modelDir, \"checkpoints\", str(modelVersions[\"version\"]), str(modelVersions[\"checkpoint\"]), str(epoch)))\n",
    "\n",
    "        print(\"Epoch: \", epoch, \".\\t Total loss: \", total_loss/len(train_loader))\n",
    "    \n",
    "    writer.flush()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = Tools()\n",
    "\n",
    "embedding_layer, tokenizer = tools.getLLM()\n",
    "vocab_size, d_model = embedding_layer.weight.size()\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}, d_model: {d_model}\")\n",
    "\n",
    "DataPath = os.path.join(os.getcwd(), os.pardir, \"data\", \"dataset2\")\n",
    "ModelPath = os.path.join(os.getcwd(), \"model\")\n",
    "h5File = os.path.join(DataPath, \"keypoints.h5\")\n",
    "csvFile = os.path.join(DataPath, \"meta.csv\")\n",
    "\n",
    "# parameters\n",
    "modelParameters = {\n",
    "    \"model\": {\n",
    "        \"version\": 1,\n",
    "        \"checkpoint\": 1\n",
    "    },\n",
    "    \"input_size\": 543*2,\n",
    "    \"output_size\": 3072,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"epochs\": 1,\n",
    "    \"logIntervals\": 10,\n",
    "    \"checkpointIntervals\": 5,\n",
    "    \"batchSize\": 32,\n",
    "    \"frameClips\": 15 * 35,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypointReader = KeypointDataset(h5Path=h5File, labelsCSV=csvFile, max_seq_len=modelParameters[\"frameClips\"])\n",
    "dataset = SignDataLoader(tokenizer, embedding_layer, keypointReader, modelParameters[\"device\"])\n",
    "dataloader = DataLoader(dataset, batch_size=modelParameters[\"batchSize\"], shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = Imitator(input_size=modelParameters[\"input_size\"], output_size=modelParameters[\"output_size\"], d_model=d_model).to(modelParameters[\"device\"])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by_keyword = 'cuda_time_total'\n",
    "\n",
    "train(model, dataloader, epochs=modelParameters[\"epochs\"], log_interval=modelParameters[\"logIntervals\"], learning_rate=modelParameters[\"learning_rate\"], modelVersions=modelParameters[\"model\"], modelDir=ModelPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
