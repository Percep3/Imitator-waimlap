{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37dea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/Code/Sign-AI/Sign-Multimodal-Language-Model/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from Classes.train.Imitator import Imitator\n",
    "from Classes.train.PositionalEncoding import PositionalEncoding\n",
    "from Classes.dataloader import KeypointDataset, SignDataLoader, collate_fn\n",
    "from Classes.utils.llm_tools import Tools\n",
    "\n",
    "from Classes.inference import MultimodalSignLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59d2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParameters = {\n",
    "    \"input_size\": 543*2,\n",
    "    \"output_size\": 3072,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"epochs\": 1000,\n",
    "    \"logIntervals\": 20,\n",
    "    \"checkpointIntervals\": 40,\n",
    "    \"batchSize\": 32,\n",
    "    \"frameClips\": 15 * 35,\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"validation_ratio\": 0.2\n",
    "}\n",
    "# model = Imitator(input_size=modelParameters[\"input_size\"], T_size=modelParameters[\"frameClips\"], output_size=modelParameters[\"output_size\"]).to(modelParameters[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5450a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.serialization.add_safe_globals([Imitator, PositionalEncoding])\n",
    "model = torch.load(\"./model/checkpoints/33/1/15/model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973395ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPath = os.path.join(os.getcwd(), os.pardir, \"data\", \"dataset2\")\n",
    "h5File = os.path.join(DataPath, \"keypoints.h5\")\n",
    "csvFile = os.path.join(DataPath, \"meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 * 2\n",
    "load_in_4bit = True\n",
    "dtype=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a9c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 15.576 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "llama_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e445d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = llama_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf14b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 15.576 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tools' object has no attribute 'collate_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m keypointReader = KeypointDataset(h5Path=h5File, labelsCSV=csvFile, max_seq_len=modelParameters[\u001b[33m\"\u001b[39m\u001b[33mframeClips\u001b[39m\u001b[33m\"\u001b[39m])[\u001b[32m0\u001b[39m]\n\u001b[32m      4\u001b[39m dataset = SignDataLoader(tokenizer, [keypointReader], modelParameters[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m test_dataloader = DataLoader(dataset, batch_size=modelParameters[\u001b[33m\"\u001b[39m\u001b[33mbatchSize\u001b[39m\u001b[33m\"\u001b[39m], shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=\u001b[43mtools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tools' object has no attribute 'collate_fn'"
     ]
    }
   ],
   "source": [
    "LOG = False\n",
    "tools = Tools()\n",
    "keypointReader = KeypointDataset(h5Path=h5File, labelsCSV=csvFile, max_seq_len=modelParameters[\"frameClips\"])[0]\n",
    "dataset = SignDataLoader(tokenizer, [keypointReader], modelParameters[\"device\"])\n",
    "test_dataloader = DataLoader(dataset, batch_size=modelParameters[\"batchSize\"], shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with state dict\n",
    "# model = Imitator(input_size=modelParameters[\"input_size\"], T_size=modelParameters[\"frameClips\"], output_size=modelParameters[\"output_size\"]).to(modelParameters[\"device\"])\n",
    "# model.load_state_dict(torch.load(\"./model/checkpoints/2/1/80\")[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f62c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cerrar las canillas durante el cepillado de dientes, de lavarse las manos, de la cara, de afeitarse, de lavar los platos, pelar papas, en lugar de dejar correr el agua.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypointReader[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80811e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1587e-04,  3.8528e-04, -1.9379e-03,  ...,  2.3937e-04,\n",
       "         -5.4550e-04,  8.8215e-05],\n",
       "        [-1.7334e-02,  5.0293e-02,  1.6212e-04,  ...,  1.3794e-02,\n",
       "          4.3640e-03,  7.2632e-03],\n",
       "        [ 7.9346e-03,  1.6113e-02,  1.7944e-02,  ...,  7.5684e-03,\n",
       "         -1.3000e-02, -4.6387e-03],\n",
       "        ...,\n",
       "        [-3.0975e-03,  2.1057e-03,  4.8828e-03,  ..., -2.0905e-03,\n",
       "         -1.2207e-03, -2.8992e-03],\n",
       "        [-3.0975e-03,  2.1057e-03,  4.8828e-03,  ..., -2.0905e-03,\n",
       "         -1.2207e-03, -2.8992e-03],\n",
       "        [-3.0975e-03,  2.1057e-03,  4.8828e-03,  ..., -2.0905e-03,\n",
       "         -1.2207e-03, -2.8992e-03]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd02f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ea038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Enumera los pasos descritos:â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "â™ª\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "No se proporcionan pasos, solo un texto vacÃ­o.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "llama_model.eval()\n",
    "mslm = MultimodalSignLM(llama_model, tokenizer, \"cuda\")\n",
    "\n",
    "text = \"Enumera los pasos descritos:\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, embeds in test_dataloader:\n",
    "        data = data.to(modelParameters[\"device\"])\n",
    "        sign_embed = model(data).to(\"cuda\")\n",
    "        sign_embed = sign_embed.to(dtype=torch.bfloat16)\n",
    "\n",
    "        # Normaliza ambos embeddings antes de calcular similitud\n",
    "        sign_embed = F.normalize(sign_embed, dim=-1)\n",
    "        embeds = F.normalize(embeds.to(sign_embed.dtype), dim=-1)\n",
    "        \n",
    "        similarity = torch.mean(torch.sum(sign_embed * embeds, dim=-1))  # ya que estÃ¡n normalizados\n",
    "        print(similarity)\n",
    "        \n",
    "        print(mslm.generate(sign_embed, text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
